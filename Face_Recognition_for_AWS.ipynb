{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:16: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-1-fd79bbd57553>:16: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting samples completed !\n"
     ]
    }
   ],
   "source": [
    "# Step 1 : Creating Data \n",
    "import cv2 \n",
    "import numpy as np\n",
    "\n",
    "# Loading Haar_face_cascade_classifier \n",
    "face_classfier=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Loading functions \n",
    "\n",
    "def face_extractor(img):\n",
    "    # This function will detect the faces and return the cropped faces\n",
    "    # If no face is detected,it returns the input image.\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_classfier.detectMultiScale(gray,1.3,5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Cropping all the faces found\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cropped_face = img[y:y+h,x:x+w]\n",
    "    return cropped_face\n",
    "\n",
    "# Initializing webcam\n",
    "cap=cv2.VideoCapture(0)\n",
    "count=0\n",
    "\n",
    "# Collect 100 samples of your face from webcam input\n",
    "while True:\n",
    "    \n",
    "    \n",
    "    ret,frame=cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        count+=1\n",
    "        face=cv2.resize(face_extractor(frame),(200,200))\n",
    "        face=cv2.cvtColor(face,cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Save file in specified directory with unique name \n",
    "        file_name_path='./dataset_faces_2/'+ str(count) + '.jpg'\n",
    "        cv2.imwrite(file_name_path,face)\n",
    "        \n",
    "        # Putting count on images and display live count\n",
    "        cv2.putText(face,str(count),(50,50),cv2.FONT_HERSHEY_COMPLEX,1,(255,255,255),2)\n",
    "        cv2.imshow('Face Cropped',face)\n",
    "    else:\n",
    "        print(\"Face not found\")\n",
    "        pass\n",
    "    \n",
    "    if cv2.waitKey(1)==13 or count==100 :\n",
    "        # 13 is the Enter key\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Collecting samples completed !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Training the model\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained successfully\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Getting the training data we previously made\n",
    "data_path='./dataset_faces_2/'\n",
    "onlyfiles=[f for f in listdir(data_path) if isfile(join(data_path,f))]\n",
    "\n",
    "# Creating arrays for training data and labels \n",
    "Training_Data, Labels = [], []\n",
    "\n",
    "# Open training images in our datapath\n",
    "# Creating a numpy array for training data \n",
    "\n",
    "for i, files in enumerate(onlyfiles):\n",
    "    image_path=data_path + onlyfiles[i]\n",
    "    images=cv2.imread(image_path,cv2.IMREAD_GRAYSCALE)\n",
    "    Training_Data.append(np.asarray(images,dtype=np.uint8))\n",
    "    Labels.append(i)\n",
    "# Creating a numpy array for both training data and labels\n",
    "Labels=np.asarray(Labels,dtype=np.int32)\n",
    "# Initialize facial recognizer\n",
    "# model=cv2.face.createLBPHFaceRecognizer()\n",
    "# NOTE : For OpenCV 3.0 use cv2.face.createLBPHFaceRecognizer()\n",
    "# pip install opencv-contrib-python\n",
    "# model=cv2.createLBPHFacerecognizer()\n",
    "\n",
    "astha_model=cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "# Training the model\n",
    "astha_model.train(np.asarray(Training_Data),np.asarray(Labels))\n",
    "print(\"Model Trained successfully\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Running our Facial Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:10: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-5-2289721c3788>:10: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance launched....!\n",
      "Volume created !\n",
      "Instance launched....!\n",
      "Volume created !\n",
      "Instance launched....!\n",
      "Volume created !\n",
      "Instance launched....!\n",
      "Volume created !\n",
      "Instance launched....!\n",
      "Volume created !\n",
      "Instance launched....!\n",
      "Volume created !\n",
      "Instance launched....!\n",
      "Volume created !\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "face_classifier=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "def face_detector(img,size=0.5):\n",
    "    # Converting image to grayscale\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_classifier.detectMultiScale(gray,1.3,5)\n",
    "    if faces is ():\n",
    "        return img, []\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(roi,(200,200))\n",
    "    return img,roi\n",
    "# Open Webcam\n",
    "cap=cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame=cap.read()\n",
    "    image, face=face_detector(frame)\n",
    "    \n",
    "    try:\n",
    "        face=cv2.cvtColor(face,cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Pass face to prediction model\n",
    "        # \"results\" comprises of a tuple containing the label and the confidence value\n",
    "        \n",
    "        results=astha_model.predict(face)\n",
    "        if results[1] < 500:\n",
    "            confidence = int( 100 * (1 - (results[1])/400) )\n",
    "            display_string = str(confidence) + '% Confident'\n",
    "            \n",
    "        cv2.putText(image, display_string, (100, 120), cv2.FONT_HERSHEY_COMPLEX, 1, (255,120,150), 2)\n",
    "        \n",
    "        if confidence > 90:\n",
    "            os.system(\"aws run ec2-instances --image-id  ami-06a0b4e3b7eb7a300 --instance-type t2.micro --subnet-id subnet --count 1 --security-group-ids sg-06af44cdc039a8b87 --subnet-id subnet-af6168c7 --key-name key > ec2.txt\")             \n",
    "            print(\"Instance launched....!\")\n",
    "            os.system(\"aws ec2 create-volume --availability-zone ap-south-1a --size 5 --volume-type gp2 --tag-specification ResourceType=volume,Tags=[{Key=face,value=volume}] > ebs.txt \")\n",
    "            print(\"Volume created !\")\n",
    "            \n",
    "            \n",
    "            time.sleep(120)\n",
    "            ec2_id=open(\"av.txt\",'r').read().split(',')[3].split(':')[1].split('\"')[1]\n",
    "            ebs_id=open(\"ebs.txt\",'r').read().split(',')[6].split(':')[1].split('\"')[1]\n",
    "            os.system(\"aws ec2 attach-volume --instance-id \" + ec2_id + \"-- volume-id\" + ebs_id +\n",
    "                     \"device /dev/xvdf\")\n",
    "            print(\"Volume attached to the instance\")\n",
    "            break\n",
    "                      \n",
    "                      \n",
    "        else: \n",
    "            cv2.putText(image, \"I dont know...who r u\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "            cv2.imshow('Face Recognition', image )\n",
    "    except:\n",
    "        cv2.putText(image, \"No Face Found\", (220, 120) , cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.putText(image, \"looking for face\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.imshow('Face Recognition', image )\n",
    "        pass\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()     \n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
